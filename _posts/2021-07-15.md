---
layout: post
title: What is Big Data?
---
Big Data ဆိုတာ ဘာလဲ?

နာမည်မှာကို ပါတဲ့အတိုင်းပဲ ကြီးမားလှသော ဒေတာ ပေါ့။ ဘယ်လောက်တောင် ကြီးလဲ? သာမန် computer software တွေနဲ့ process လုပ်လို့မရနိုင်လောက်အောင် ကြီးတယ်။ ကိန်းဂဏန်းနဲ့ ပြောရမယ်ဆိုရင် 1999 ခုနှစ်လောက်တုန်းက 1 GB သည် Big Data ပဲ။ အဲ့တုန်းက တစ်ကမ္ဘာလုံးမှာ ရှိသမျှ Data စုစုပေါင်းမှ 1.5 exabytes ပဲ ရှိတယ်လို့ ဆိုတယ်။ 2006 ရောက်တဲ့အခါ Data စုစုပေါင်းဟာ 160 exabytes အထိ ရှိလာပြီး 1 GB ဟာလည်း Big Data မဟုတ်တော့။ ဒီတော့ သင်္ချာဖော်မြူလာနဲ့ ပြောရရင် တစ်ကမ္ဘာလုံးမှာ ရှိသမျှ data ပမာဏရဲ့  1000³ ပုံပုံလို့ တစ်ပုံ ဆိုရင် Big Data လို့ ပြောလို့ရပြီ။


ဒါကတော့ Big Data နဲ့ ပက်သက်တဲ့ အကြမ်းဖျဉ်း သဘောတရားတွေပါ။ 
မြင်သာအောင် ဉပမာ ပြောရရင်

စားသောက်ဆိုင် တစ်ဆိုင် ရှိတယ် ဆိုကြပါစို့။

စားဖိုမှုး တစ်ယောက်ပဲ ရှိပြီး တစ်နေ့ကို လည်း ဟင်း ပွဲ (၂၀) လောက်ပဲ ရောင်းရတယ်။ ဒီလိုနဲ့ လုပ်ငန်းလည်ပတ်နေရင်း Facebook Page မှာ ရန်ကုန်မြို့တွင်း Delivery နဲ့ ပို့ပေးပါမယ်လို့ ကြေညာလိုက်တာ တစ်နေ့ကို ဟင်းပွဲ (၁၀၀) လောက် အော်ဒါ တက်လာတယ်။ ဒါပေမယ့် စားဖိုမှုးက တစ်နေ့ကို ဟင်းပွဲ (၂၀) ပဲ ချက်နိုင်တယ်။ 

ဒါနဲ့ ဆိုင်ပိုင်ရှင်က နောက်ထပ် စားဖိုမှုး (၄) ယောက် ထပ်ငှားလိုက်တယ်။ ဒါပေမယ့်လည်း ပြဿနာ က ပြေလည် မသွားသေးဘူး။ အသားငါးတွေ ထားတဲ့ ရေခဲသေတ္တာ နဲ့ ချက်ပြုတ်ရမယ့် အိုးခွက်ပန်းကန် တွေက မလောက်ဘူး ဖြစ်နေတယ်။ ဒါနဲ့ လိုတဲ့ ရေခဲသေတ္တာ ၊ အိုးခွက်ပန်းကန် နဲ့ အသားငါး ဟင်းချက်စရာတွေ ထပ်ဝယ်ပေးလိုက်တယ်။

ဒါလည်း အဆင်မပြေသေး။ ဟင်းတစ်ပွဲ အော်ဒါတက်လာရင် ဘယ် ရေခဲသေတ္တာက ပစ္စည်းတွေ ယူသုံးပြီး ဘယ်သူချက်ရမယ် ဘာညာသာရကာ ညွှန်ကြားပေးနိုင်တဲ့ မန်နေဂျာ တစ်ယောက်လိုလာရော။ အိုကေပေါ့ မန်နေဂျာ တစ်ယောက် ခန့်လိုက်တယ်။ အဲ ဒီမှာမှ စနစ်က စပြီး ပုံမှန်လည်ပတ်တော့တယ်။

နောက် ဟင်းပွဲကြီး အော်ဒါ ရောက်လာလို့ စားဖိုမှုး တစ်ယောက်တည်း မချက်မနိုင်တဲ့အခါမှာ သင့်တင့်တဲ့ စားဖိုမှုး နှစ်ယောက် သုံးယောက်ကို တွဲပေးပြီး မင်းကတော့ ရေခဲသေတ္တာ အမှတ်(၁)ထဲက ကြက်သွန်ဖြူနဲ့ ငရုပ်သီးတွေယူပြီး ကြက်သွန်ဖြူနွှာ၊ ငရုပ်သီးထောင်း၊ မင်းကတော့ ရေခဲသေတ္တာ အမှတ်(၂)ထဲက ရေသန့်ယူပြီး ရေနွေးအိုးတယ်၊ မင်းကတော့ ရေခဲသေတ္တာ အမှတ်(၃) က ကြက်သားယူပြီး ကြက်သားကြော်  ပြီးရင် ငါ့ကို ပြန်ပြော စသည်ဖြင့် မန်နေဂျာက  စီမံပေးတယ်။ ဒီမှာ အားလုံးအဆင်ပြေသွားတော့လေသတည်းပေါ့။


ဇာက်ပေါင်းသော်  စားဖိုမှုးဆိုတာ Computer ပါပဲ။ နောက် ရေခဲသေတ္တာ ဆိုတာကတော့ Storage ပဲ ဆိုကြပါဆို၊ အသားငါး နဲ့ ဟင်းချက်ပစ္စည်းတွေဟာ အမျိုးစုံသော Data တွေပေါ့။ ဟင်းပွဲ အော်ဒါ ကတော့ စောနကလိုမျိုး အောက်မှာ နမူနာ ပြထားသလို City Mart မှာ ရောင်းအားအကောင်းဆုံး ပစ္စည်းတွေ ရှာခိုင်းသလိုမျိုး instruction တွေ။ မန်နေဂျာကတော့ Hadoop လိုမျိုး Big Data Framework ပေါ့။

Manager နဲ့ တူတဲ့ Big Data Framework တွေသည် ဟင်းချက်ပစ္စည်းလိုမျိုး မျိုးစုံတဲ့ Data တွေ import လုပ်တဲ့အခါမှာ ရေခဲသေတ္တာ တစ်ခုတည်းမှာ စုထည့်လို့ မရသလိုမျိုး distributed storage system တစ်ခု (ဉပမာ Apache Hadoop’s HDFS  ) သုံးပြီး data တွေကို မျှ သိမ်းရတယ်။ ဘယ်သူ ဘယ်အပိုင်းကို ချက် ဘယ်သူက ဟင်းပွဲပြင် စတာတွေကို Apache Hadoop’s YARN လိုမျိုး Resource Negotiator တွေက စီမံပေးတယ်။


ဒီတော့ Big Data ဆိုတာ ကွန်ပျူတာ တစ်လုံးထဲမှာ သိမ်းဖို့၊ သုံးဖို့ မဖြစ်နိုင်တဲ့ ပမာဏလို့ အကြမ်းဖျဉ်း မှတ်ကြတာပေါ့။ Big Data နဲ့ ရိုးရိုး Data နဲ့သည် V (၃) လုံးအပေါ် မူတည်ပြီး ကွာခြားသွားတယ်။

1. Volume
ဒါကတော့ ကိုင်တွယ်ရမယ့် Data ပမာဏပေါ့။ Data တစ်ခုကို ကွန်ပျူတာ တစ်လုံးတည်းမှာ သိမ်းဖို့ ၊ process လုပ်ဖို့ မဖြစ်နိုင်တဲ့အခါ Cluster နည်းပညာသုံးပြီး ကွန်ပျူတာ အများကြီးမှာ ခွဲခြမ်းစိတ်ဖြာပြီး သိမ်းရတယ်။ process လုပ်ရတယ်။

2. Velocity
ကိုယ့် စနစ်ထဲကို Data Import လုပ်တဲ့ ပုံစံတွေလို့ ပြောရမလားပဲ။ Batch လိုမျိုး အစုလိုက် အပြုံလိုက် ထည့်တာ ရှိမယ်။ အချိန်အပိုင်းအခြားအလိုက် ထည့်တာရှိမယ်၊ နောက် GPS Sensor လိုမျိုး IoT Device တွေကနေ အချိန်နဲ့ တစ်ပြေးညီ real-time ထည့်တာတွေ ရှိမယ်။ ဒါတွေဟာလည်း ရိုးရိုး Data စနစ်ထက် Big Data မှာက ပိုပြီး များပြား ရှုပ်ထွေးလာတယ်။

3. Variety
ပုံမှန် စနစ်တစ်ခုမှာက ထည့်သွင်း / သိမ်းစည်း တဲ့ data format သည် တိတိကျကျ ရှိပြီးသားတွေ များတယ်။ Big Data မှာတော့ Data မျိုးစုံ သိမ်းကြတယ်။ Application / Server Logs တွေ၊ Social Media က Feeds တွေ၊ Third party Api က data တွေ စသည်ဖြင့် အမျိုးစုံ ကြိုးခုန်တယ်။ နောက် unstructured data တွေဖြစ်တဲ့  images, video files, and audio recordings , pdf စတာတွေကိုလည်း သိမ်းရတယ်။ process လုပ်ရတယ်။

Big Data Life Cycle
အကြမ်းဖျဉ်း အားဖြင့် Big Data နဲ့ ပတ်သက်တဲ့ process တွေကို ဒီလို လေးခု ခွဲနိုင်တယ်။

1. Ingesting data into the system
Raw Data တွေကို သင့်တင့်လျောက်ပတ်အောင် ပြင်ဆင်ပြီး ကိုယ့်ရဲ့ system ထဲကို ထည့်သွင်း တဲ့အပိုင်း။ ETL ခေါ်မှာပေါ့  raw data တွေကို extract, transform, and load လုပ်ပြီး ကိုယ့်စနစ်အတွက် လိုတာတွေ စစ်ယူ၊ မလိုတာတွေ ဖယ်ထုတ်တဲ့ အပိုင်း ဆိုပါတော့။

2. Persisting the Data in Storage
ရိုးရိုးရှင်းရှင်း တွေးမယ်ဆိုရင်တော့ data တွေကို harddrive မှာ သိမ်းတယ်ပေါ့။ ခက်တာက ရိုးရိုး drive တစ်ခုတည်းမှာ မဆန့်လေတော့ Distributed File System တစ်ခုသုံးပြီး သိမ်းမှ ရတယ်။ ဉပမာ   Apache Hadoop’s HDFS 

3. Computing and Analyzing Data
ဉပမာ တစ်နိုင်ငံလုံးက City Mart တွေမှာ ဒီ Covid Third Wave အတွင်း ရောင်းအားအကောင်းဆုံး ကုန်ပစ္စည်းစာရင်းကို ပြပေးပါ ဆိုတာမျိုး။  တကယ့်တွက်တဲ့ချက်တဲ့အပိုင်းတွေမှာလည်း ကိုင်တွယ်ရမယ့်  data ပမာဏ က များတော့ processing power လည်း အများကြီး လိုတယ်။ ထုံးစံအတိုင်းပဲ cluster တွေ ခွဲပြီးတော့ပဲ အပိုင်းပိုင်းခွဲတွက်ကြရတယ်၊ ပြီးမှ ရလာတဲ့ result ကို ပြန်ပေါင်းပြီး ပြပေး။ ဒါမျိုးကို Apache Hadoop’s MapReduce သုံးပြီး တွက်ကြသတဲ့။

4. Visualizing the Results
တွက်ချက်ပြီး ရလာတဲ့ result ကို သာမန်လူ မြင်သာအောင် Visualize လုပ်ပြီး ပြပေးတာမျိုး။ ဉပမာ ရောင်းအားအကောင်းဆုံး ကုန်ပစ္စည်း(၁၀) မျိုး ဆိုပြီး table နဲ့ မပြပဲ bar chart / pie chart တစ်ခုခု နဲ့ ပြရင် မျက်လုံးထဲ ပိုပြီး မြင်သာတာပေါ့။


ဒါကတော့ Big Data နဲ့ ပက်သက်တဲ့ အကြမ်းဖျဉ်း သဘောတရားတွေပါ။ 