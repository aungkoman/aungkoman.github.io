<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OCR for Myanmar Printed Documents (OCRMPD)</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 900px;
            margin: 20px auto;
            padding: 0 15px;
            background-color: #f9f9f9;
        }
        .container {
            background-color: #fff;
            padding: 25px;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }
        h1, h2, h3, h4 {
            color: #0056b3;
        }
        h1, h2 {
            border-bottom: 2px solid #e0e0e0;
            padding-bottom: 10px;
            margin-top: 0;
        }
        h4 {
            margin-bottom: 10px;
            color: #004085;
        }
        .section {
            margin-bottom: 30px;
        }
        .step {
            background-color: #f0f7ff;
            border-left: 4px solid #007bff;
            padding: 15px;
            margin-top: 15px;
            border-radius: 4px;
        }
        .step h3 {
            margin-top: 0;
        }
        canvas {
            border: 1px solid #ccc;
            max-width: 100%;
            height: auto;
            margin-top: 10px;
            background-color: #fdfdfd;
        }
        #output-area {
            border: 1px solid #ccc;
            background-color: #fdfdfd;
            padding: 15px;
            min-height: 100px;
            border-radius: 4px;
            font-size: 1.2em;
            white-space: pre-wrap;
            margin-top: 10px;
        }
        .controls {
            display: flex;
            gap: 15px;
            align-items: center;
            flex-wrap: wrap;
        }
        button {
            background-color: #007bff;
            color: white;
            border: none;
            padding: 12px 20px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 1em;
            transition: background-color 0.3s;
        }
        button:hover {
            background-color: #0056b3;
        }
        input[type="file"] {
            border: 1px solid #ccc;
            padding: 8px;
            border-radius: 5px;
        }
        #spinner {
            display: none;
            border: 4px solid #f3f3f3;
            border-top: 4px solid #007bff;
            border-radius: 50%;
            width: 24px;
            height: 24px;
            animation: spin 1s linear infinite;
        }
        #segments-output-container {
            margin-top: 15px;
            padding: 10px;
            border: 1px dashed #b0c4de;
            background-color: #f8f9fa;
            min-height: 50px;
            border-radius: 4px;
        }
        .line-container {
            margin-bottom: 15px;
            padding-bottom: 10px;
            border-bottom: 2px solid #e0e0e0;
        }
        .line-container:last-child {
            border-bottom: none;
            margin-bottom: 0;
            padding-bottom: 0;
        }
        .line-container div {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
        }
        .line-container canvas {
            border: 1px solid #999;
            background-color: #fff;
        }
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
    </style>
</head>
<body>

<div class="container">
    <h1>Optical Character Recognition System for Myanmar Printed Documents (OCRMPD) üá≤üá≤</h1>
    <p>This is a web-based demonstration of the OCRMPD system. Upload an image of Myanmar text to see a simulation of the recognition process based on the proposed algorithms.</p>

    <div class="section">
        <h2>1. Upload Document Image</h2>
        <div class="controls">
            <input type="file" id="image-loader" name="imageLoader" accept="image/*">
            <select id="sample-image-selector">
                <option value="">-- Select a sample image --</option>
                <option value="2021-02-08-Announcement_No.1-bu-red.pdf-page_1-line_10">Sample 1 (·Äï·Äº·Ää·Ä∫·Äë·Ä±·Ä¨·ÄÑ·Ä∫·ÄÖ·ÄØ·Äú·Äæ·Äê·Ä∫·Äê·Ä±·Ä¨·Ä∫·ÄÄ·Ä≠·ÄØ·Äö·Ä∫·ÄÖ·Ä¨·Ä∏·Äï·Äº·ÄØ·ÄÄ·Ä±·Ä¨·Ä∫·Äô·Äê·ÄÆ)</option>
                <option value="2021-04-12_Razel-Bec_update-bu.pdf-page_1-line_10">Sample 2</option>
                <option value="2021-06-11-KHRG_joint_statement_bu.pdf-page_1-line_10">Sample 3</option>
            </select>
            <button id="process-btn">Recognize Text</button>
            <div id="spinner"></div>
        </div>
        <canvas id="image-canvas"></canvas>
    </div>

    <div class="section">
        <h2>2. Algorithm Processing Stages</h2>

        <div class="step">
            <h3>Stage A: Segmentation Algorithm</h3>
            <p>The system first performs <strong>Line Segmentation</strong> using a horizontal projection histogram to isolate each line of text (blue boxes). Then, for each line, it performs <strong>Character Segmentation</strong> with the vertical histogram and structural analysis logic to isolate individual characters (red boxes).</p>
            <canvas id="segment-canvas"></canvas>
            
            <h4>Extracted Segments (Grouped by Line):</h4>
            <div id="segments-output-container">
                <p style="color: #666;">Segments from the original image will be displayed here after processing.</p>
            </div>
        </div>

        <div class="step">
            <h3>Stage B: Feature Extraction Algorithm</h3>
            <p>After segmentation, each character image is normalized. A hybrid statistical method extracts unique features using <strong>Zone Density</strong> (pixel density in 3 horizontal zones) and <strong>Projection Area</strong> (area from top/bottom/left/right profiles). This creates a feature vector for each character.</p>
        </div>

        <div class="step">
            <h3>Stage C: Classification Algorithm</h3>
            <p>Finally, a <strong>Hierarchical Multi-class Support Vector Machine (SVM)</strong> classifier receives the feature vector. The hierarchical model efficiently navigates the large and visually similar Myanmar character set to determine the final text output.</p>
        </div>
    </div>

    <div class="section">
        <h2>3. Recognized Text Output</h2>
        <div id="output-area">The recognized text will appear here...</div>
    </div>
</div>

<script>
document.addEventListener('DOMContentLoaded', () => {
    const imageLoader = document.getElementById('image-loader');
    const processBtn = document.getElementById('process-btn');
    const imageCanvas = document.getElementById('image-canvas');
    const segmentCanvas = document.getElementById('segment-canvas');
    const segmentsContainer = document.getElementById('segments-output-container');
    const outputArea = document.getElementById('output-area');
    const spinner = document.getElementById('spinner');
    const sampleImageSelector = document.getElementById('sample-image-selector');

    const ctxImage = imageCanvas.getContext('2d');
    const ctxSegment = segmentCanvas.getContext('2d');
    let originalImage = null;
    let currentGroundTruth = '';

    imageLoader.addEventListener('change', (e) => {
        if (e.target.files[0]) {
            const reader = new FileReader();
            reader.onload = (event) => {
                loadImage(event.target.result);
            }
            reader.readAsDataURL(e.target.files[0]);
            sampleImageSelector.value = ''; // Clear sample selection
            currentGroundTruth = ''; // Clear ground truth for uploaded image
        }
    });

    sampleImageSelector.addEventListener('change', (e) => {
        const selectedValue = e.target.value;
        if (selectedValue) {
            imageLoader.value = ''; // Clear file input
            const imagePath = `./ocr-dataset/burmese_ocr_data/cleaned/cleaned/pic/${selectedValue}.png`;
            const gtPath = `./ocr-dataset/burmese_ocr_data/cleaned/cleaned/gt/${selectedValue}.gt.txt`;
            
            // Load image
            loadImage(imagePath);

            // Load ground truth
            fetch(gtPath)
                .then(response => response.text())
                .then(text => {
                    currentGroundTruth = text.trim();
                })
                .catch(error => {
                    console.error('Error loading ground truth:', error);
                    currentGroundTruth = 'Error loading ground truth.';
                });
        }
    });

    function loadImage(src) {
        const img = new Image();
        img.onload = () => {
            originalImage = img;
            const maxWidth = 800;
            const scale = Math.min(1, maxWidth / img.width);
                const w = img.width * scale;
                const h = img.height * scale;

                imageCanvas.width = w;
                imageCanvas.height = h;
                segmentCanvas.width = w;
                segmentCanvas.height = h;

                ctxImage.drawImage(img, 0, 0, w, h);
                ctxSegment.clearRect(0, 0, w, h);
                segmentsContainer.innerHTML = '<p style="color: #666;">Segments from the original image will be displayed here after processing.</p>';
                outputArea.textContent = 'The recognized text will appear here...';
            }
            img.src = src;
        }

    processBtn.addEventListener('click', () => {
        if (!originalImage) {
            alert('Please upload an image or select a sample first.');
            return;
        }

        spinner.style.display = 'block';
        outputArea.textContent = 'Processing...';
        segmentsContainer.innerHTML = '';

        setTimeout(() => {
            const { binarizedImageData, scale } = preprocessImage(originalImage);
            
            // Draw original image on segmentation canvas to draw boxes over
            ctxSegment.drawImage(originalImage, 0, 0, segmentCanvas.width, segmentCanvas.height);

            // Run the full segmentation pipeline
            processSegmentation(binarizedImageData, originalImage, scale);

            simulateRecognition(currentGroundTruth);
            spinner.style.display = 'none';
        }, 1500);
    });

    function preprocessImage(img) {
        const tempCanvas = document.createElement('canvas');
        tempCanvas.width = img.width;
        tempCanvas.height = img.height;
        const tempCtx = tempCanvas.getContext('2d');
        tempCtx.drawImage(img, 0, 0);
        
        const imageData = tempCtx.getImageData(0, 0, tempCanvas.width, tempCanvas.height);
        const data = imageData.data;

        for (let i = 0; i < data.length; i += 4) {
            const avg = (data[i] + data[i + 1] + data[i + 2]) / 3;
            const color = avg > 128 ? 255 : 0;
            data[i] = color; data[i + 1] = color; data[i + 2] = color;
        }
        
        const scale = segmentCanvas.width / img.width;
        return { binarizedImageData: imageData, scale: scale };
    }

    // Main segmentation controller
    function processSegmentation(imageData, sourceImage, scale) {
        // 1. Segment into lines
        const lines = segmentLines(imageData);
        
        // 2. For each line, segment characters
        lines.forEach((line, index) => {
            // Create a container for this line's output
            const lineContainer = document.createElement('div');
            lineContainer.className = 'line-container';
            const lineTitle = document.createElement('h4');
            lineTitle.textContent = `Line ${index + 1}`;
            const charContainer = document.createElement('div');
            
            lineContainer.appendChild(lineTitle);
            lineContainer.appendChild(charContainer);
            segmentsContainer.appendChild(lineContainer);

            // Draw line bounding box
            ctxSegment.strokeStyle = 'rgba(0, 0, 255, 0.8)';
            ctxSegment.lineWidth = 2;
            ctxSegment.strokeRect(0, line.startY * scale, segmentCanvas.width, (line.endY - line.startY) * scale);

            // Segment characters within this line
            segmentCharactersInLine(imageData, sourceImage, scale, line, charContainer);
        });
    }

    // Step 1: Find horizontal lines
    function segmentLines(imageData) {
        const { width, height, data } = imageData;
        const horizontalProjection = new Array(height).fill(0);

        for (let y = 0; y < height; y++) {
            for (let x = 0; x < width; x++) {
                if (data[(y * width + x) * 4] === 0) { // Black pixel
                    horizontalProjection[y]++;
                }
            }
        }

        const lines = [];
        let inLine = false;
        let lineStartY = 0;
        const minLineHeight = 5; // Ignore very small noise

        for (let y = 0; y < height; y++) {
            if (horizontalProjection[y] > 0 && !inLine) {
                inLine = true;
                lineStartY = y;
            } else if ((horizontalProjection[y] === 0 || y === height - 1) && inLine) {
                inLine = false;
                if (y - lineStartY > minLineHeight) {
                    lines.push({ startY: lineStartY, endY: y });
                }
            }
        }
        return lines;
    }

    // Step 2: Find characters within a given line
    function segmentCharactersInLine(imageData, sourceImage, scale, line, container) {
        const { width, data } = imageData;
        const { startY, endY } = line;
        const verticalProjection = new Array(width).fill(0);

        // Calculate vertical histogram ONLY for the current line
        for (let x = 0; x < width; x++) {
            for (let y = startY; y < endY; y++) {
                if (data[(y * width + x) * 4] === 0) {
                    verticalProjection[x]++;
                }
            }
        }
        
        ctxSegment.strokeStyle = 'rgba(255, 0, 0, 0.9)';
        ctxSegment.lineWidth = 1;
        let inChar = false;
        let startX = 0;
        const minCharWidth = 3;

        for (let x = 0; x < width; x++) {
            const isPixelPresent = verticalProjection[x] > 0;
            if (isPixelPresent && !inChar) {
                startX = x;
                inChar = true;
            } else if ((!isPixelPresent || x === width - 1) && inChar) {
                if (x - startX > minCharWidth) {
                    const endX = x;
                    const segmentWidth = endX - startX;
                    const segmentHeight = endY - startY;

                    // Draw character bounding box
                    ctxSegment.strokeRect(startX * scale, startY * scale, segmentWidth * scale, segmentHeight * scale);
                    
                    // Create and append the segment canvas
                    const segmentCanvas = document.createElement('canvas');
                    segmentCanvas.width = segmentWidth;
                    segmentCanvas.height = segmentHeight;
                    const segmentCtx = segmentCanvas.getContext('2d');
                    
                    segmentCtx.drawImage(
                        sourceImage,
                        startX, startY,           // Source X, Y
                        segmentWidth, segmentHeight, // Source Width, Height
                        0, 0,                     // Destination X, Y
                        segmentWidth, segmentHeight   // Destination Width, Height
                    );
                    container.appendChild(segmentCanvas);
                }
                inChar = false;
            }
        }
    }

    function simulateRecognition(groundTruth) {
        outputArea.textContent = groundTruth || "No ground truth available for this sample.";
    }
});
</script>

</body>
</html>