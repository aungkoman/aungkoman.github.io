<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OCR for Myanmar Printed Documents (OCRMPD)</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 900px;
            margin: 20px auto;
            padding: 0 15px;
            background-color: #f9f9f9;
        }
        .container {
            background-color: #fff;
            padding: 25px;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }
        h1, h2 {
            color: #0056b3;
            border-bottom: 2px solid #e0e0e0;
            padding-bottom: 10px;
            margin-top: 0;
        }
        .section {
            margin-bottom: 30px;
        }
        .step {
            background-color: #f0f7ff;
            border-left: 4px solid #007bff;
            padding: 15px;
            margin-top: 15px;
            border-radius: 4px;
        }
        .step h3 {
            margin-top: 0;
            color: #0056b3;
        }
        canvas {
            border: 1px solid #ccc;
            max-width: 100%;
            height: auto;
            margin-top: 10px;
            background-color: #fdfdfd;
        }
        #output-area {
            border: 1px solid #ccc;
            background-color: #fdfdfd;
            padding: 15px;
            min-height: 100px;
            border-radius: 4px;
            font-size: 1.2em;
            white-space: pre-wrap;
            margin-top: 10px;
        }
        .controls {
            display: flex;
            gap: 15px;
            align-items: center;
            flex-wrap: wrap;
        }
        button {
            background-color: #007bff;
            color: white;
            border: none;
            padding: 12px 20px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 1em;
            transition: background-color 0.3s;
        }
        button:hover {
            background-color: #0056b3;
        }
        input[type="file"] {
            border: 1px solid #ccc;
            padding: 8px;
            border-radius: 5px;
        }
        #spinner {
            display: none;
            border: 4px solid #f3f3f3;
            border-top: 4px solid #007bff;
            border-radius: 50%;
            width: 24px;
            height: 24px;
            animation: spin 1s linear infinite;
        }
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
    </style>
</head>
<body>

<div class="container">
    <h1>Optical Character Recognition System for Myanmar Printed Documents (OCRMPD) ðŸ‡²ðŸ‡²</h1>
    <p>This is a web-based demonstration of the OCRMPD system. Upload an image of Myanmar text to see a simulation of the recognition process based on the proposed algorithms.</p>

    <div class="section">
        <h2>1. Upload Document Image</h2>
        <div class="controls">
            <input type="file" id="image-loader" name="imageLoader" accept="image/*">
            <button id="process-btn">Recognize Text</button>
            <div id="spinner"></div>
        </div>
        <canvas id="image-canvas"></canvas>
    </div>

    <div class="section">
        <h2>2. Algorithm Processing Stages</h2>

        <div class="step">
            <h3>Stage A: Segmentation Algorithm</h3>
            <p>The system first isolates individual characters. A vertical projection histogram (X-Y cut) makes initial cuts. Then, a crucial <strong>Structural Analysis</strong> step refines these cuts, checking pixel density and connected components (CCs) to accurately segment complex, overlapping Myanmar characters.</p>
            <canvas id="segment-canvas"></canvas>
        </div>

        <div class="step">
            <h3>Stage B: Feature Extraction Algorithm</h3>
            <p>After segmentation, each character image is normalized. A hybrid statistical method extracts unique features using <strong>Zone Density</strong> (pixel density in 3 horizontal zones) and <strong>Projection Area</strong> (area from top/bottom/left/right profiles). This creates a feature vector for each character.</p>
        </div>

        <div class="step">
            <h3>Stage C: Classification Algorithm</h3>
            <p>Finally, a <strong>Hierarchical Multi-class Support Vector Machine (SVM)</strong> classifier receives the feature vector. The hierarchical model efficiently navigates the large and visually similar Myanmar character set to determine the final text output.</p>
        </div>
    </div>

    <div class="section">
        <h2>3. Recognized Text Output</h2>
        <div id="output-area">The recognized text will appear here...</div>
    </div>
</div>

<script>
document.addEventListener('DOMContentLoaded', () => {
    const imageLoader = document.getElementById('image-loader');
    const processBtn = document.getElementById('process-btn');
    const imageCanvas = document.getElementById('image-canvas');
    const segmentCanvas = document.getElementById('segment-canvas');
    const outputArea = document.getElementById('output-area');
    const spinner = document.getElementById('spinner');

    const ctxImage = imageCanvas.getContext('2d');
    const ctxSegment = segmentCanvas.getContext('2d');
    let originalImage = null;

    imageLoader.addEventListener('change', (e) => {
        const reader = new FileReader();
        reader.onload = (event) => {
            const img = new Image();
            img.onload = () => {
                originalImage = img;
                // Set canvas size to image size
                imageCanvas.width = img.width;
                imageCanvas.height = img.height;
                segmentCanvas.width = img.width;
                segmentCanvas.height = img.height;

                // Draw original image
                ctxImage.drawImage(img, 0, 0);
                // Clear other canvases/outputs
                ctxSegment.clearRect(0, 0, segmentCanvas.width, segmentCanvas.height);
                outputArea.textContent = 'The recognized text will appear here...';
            }
            img.src = event.target.result;
        }
        if (e.target.files[0]) {
            reader.readAsDataURL(e.target.files[0]);
        }
    });

    processBtn.addEventListener('click', () => {
        if (!originalImage) {
            alert('Please upload an image first.');
            return;
        }

        spinner.style.display = 'block';
        outputArea.textContent = 'Processing...';

        // Simulate the OCR pipeline with a delay to show processing
        setTimeout(() => {
            // 1. Pre-processing (Binarization)
            const imageData = preprocessImage(originalImage);

            // 2. Segmentation (Simplified Simulation)
            // Draw the preprocessed image on the segmentation canvas
            ctxSegment.putImageData(imageData, 0, 0);
            // Simulate segmentation by drawing boxes
            simulateSegmentation(imageData);

            // 3. Feature Extraction & Classification (Simulated)
            // This part is heavily simplified as it requires a trained model.
            // We'll just output a sample text.
            simulateRecognition();

            spinner.style.display = 'none';
        }, 1500); // 1.5 second delay for effect
    });

    function preprocessImage(img) {
        // Draw image to a temporary canvas to get pixel data
        const tempCanvas = document.createElement('canvas');
        tempCanvas.width = img.width;
        tempCanvas.height = img.height;
        const tempCtx = tempCanvas.getContext('2d');
        tempCtx.drawImage(img, 0, 0);

        const imageData = tempCtx.getImageData(0, 0, tempCanvas.width, tempCanvas.height);
        const data = imageData.data;

        // Simple grayscale and thresholding (binarization)
        for (let i = 0; i < data.length; i += 4) {
            const avg = (data[i] + data[i + 1] + data[i + 2]) / 3;
            const color = avg > 128 ? 255 : 0; // Threshold
            data[i] = color;     // red
            data[i + 1] = color; // green
            data[i + 2] = color; // blue
        }
        return imageData;
    }

    function simulateSegmentation(imageData) {
        // This is a highly simplified simulation of the vertical projection method.
        // The real algorithm is much more complex with structural analysis.
        const { width, height, data } = imageData;
        const verticalProjection = new Array(width).fill(0);

        // Calculate vertical histogram
        for (let x = 0; x < width; x++) {
            for (let y = 0; y < height; y++) {
                const pixelIndex = (y * width + x) * 4;
                if (data[pixelIndex] === 0) { // Black pixel
                    verticalProjection[x]++;
                }
            }
        }

        // Find gaps to segment characters (simple version)
        ctxSegment.strokeStyle = 'rgba(255, 0, 0, 0.7)';
        ctxSegment.lineWidth = 1;
        let inChar = false;
        let startX = 0;

        for (let x = 0; x < width; x++) {
            if (verticalProjection[x] > 2 && !inChar) {
                startX = x;
                inChar = true;
            } else if (verticalProjection[x] <= 1 && inChar) {
                // End of a character segment
                ctxSegment.strokeRect(startX - 1, 0, x - startX + 2, height);
                inChar = false;
            }
        }
    }

    function simulateRecognition() {
        // In a real application, this function would send feature vectors
        // to a trained SVM model. Here, we just return a placeholder result.
        outputArea.textContent = " à¤¡à¥‡à¤®à¥‹ á€…á€¬á€žá€¬á€¸á€€á€­á€¯ á€¤á€”á€±á€›á€¬á€á€½á€„á€º á€•á€¼á€žá€•á€«á€™á€Šá€ºá‹\n" +
            "á€¤á€žá€Šá€ºá€™á€¾á€¬ á€™á€¼á€”á€ºá€™á€¬á€…á€¬á€•á€¯á€¶á€”á€¾á€­á€•á€ºá€…á€”á€…á€ºá€¡á€á€½á€€á€º á€žá€›á€¯á€•á€ºá€•á€¼á€á€…á€ºá€á€¯á€–á€¼á€…á€ºá€žá€Šá€ºá‹";
    }
});
</script>

</body>
</html>